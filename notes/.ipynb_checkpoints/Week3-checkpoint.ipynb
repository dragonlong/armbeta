{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Demo 目标\n",
    "为机械臂进行物体的实时抓取进行整体方案的调研与算法的测试，并基于ROS测试和编写相关代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Demo1 展示\n",
    "ICRA 2017 Paper: [Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge ](https://arxiv.org/pdf/1609.09475v2.pdf)\n",
    "\n",
    "[项目网站，**快来点我** ~][hover]\n",
    "[hover]: http://apc.cs.princeton.edu/ \"好，现在单击吧\"\n",
    "#### 2.1 RGB-D 数据获取与分割\n",
    "演示内容：\n",
    "\n",
    "1. SR300 相机特性\n",
    "\n",
    "   Get ready the driver and ROS wrapper to the realsense SR300 camera;\n",
    "\n",
    "2. rgb-d 数据流获取与基于神经网络的实时图像分割\n",
    "\n",
    "   Rgb-d data capturing as a stream topic, and the Seg2D network would subscribe to it and show real-time \n",
    "   segmentation;(Mask_rcnn_ros node could capture images itself, or read images from directory)\n",
    "\n",
    "3. ROS 客户端触发进行同步的图像、相机信息采集\n",
    "\n",
    "   the camera nodes needs to get a client's request to save the rgb-d images from one scene and multiple views\n",
    "   together with camera infos(intristics, extrisics) into disk; (it might even call the Seg2D service to segment images)\n",
    "   现在的设想是，创建一个单独的package来提供采集图像，存储图像与相机信息的ROS service，输入是存储的路径与触发信号；\n",
    "\n",
    "\n",
    "#### 2.2 From 2D Mask to 3D 检测\n",
    "1. 深度相机+rgb相机的内矩阵，外矩阵标定与采集；\n",
    "2. 对于抓取物，预先进行扫描，获取.ply的点云模型， 以及收纳盒本身的点云模型；\n",
    "2. 每个物体均获取相应的多个视角下的分割好的二值掩板；\n",
    "3. 去除背景，获取分割好的点云，去噪，对每一个物体用K-means 分类，ICP 做模型匹配，输出位姿；\n",
    "\n",
    "演示内容：\n",
    "- python/MATLAB 读取与显示模型的3D点云；\n",
    "- 多视角3维重建与模型匹配，点云分割；\n",
    "\n",
    "Note:(MATLAB with 30 trial days, need to get several toolboxes + add-ons ready)\n",
    "\n",
    "#### 2.3 怎样进行实际的抓取演示+要做的准备\n",
    "流程是：机械臂前端固定相机，进行手眼标定，预先扫描并保存物体与盒子的点云模型; 在进行抓取之前，会在18个位置（对于物品重叠较少的情形，采集视角可以更少）连续调用图像采集的** ROS服务**，每个视角存储一张彩色图+深度图（最好保存已配准[align]好的），以及输出两个相机的$3*3$内矩阵到camera.info.txt，同时调取相机相对于世界坐标系进行位姿解算的**ROS服务**，解算出每个相机位姿角并存储到.txt；当图像保存好之后，给出存储路径并调用**Seg2D**神经网络的**ROS服务**进行图像的分割并把对应每个物体的掩板按物品名称存储到对应的文件夹；当所有的图像、二值分割掩板[binary mask]以及信息收集到位之后，调用**Align3D**的ROS服务，检测每个物体的3D位姿，并输出结果到.txt文件，以及发布物体位姿信息；机械臂配合进行运动规划，夹取物体；\n",
    "\n",
    "- 编写神经网络分割的ROS服务；\n",
    "  目前是可以直接检测获取到的\n",
    "  The Seg2D network needs to be built as a service server, which could seg images \n",
    "  in the given tmp-path and save the segmented results（binary mask for each object）, \n",
    "  to be called by the camera node or other nodes\n",
    "\n",
    "- 编写3维匹配算法包[Align3D]基于MATLAB的ROS服务以及消息转换；\n",
    "  （目前是demo.m直接调取存储好的图像以及相机内外参信息，直接输出到结果到.txt 文件，并进行可视化的显示；）\n",
    "\n",
    "- 对于新的物体与盒子，分别重新采集3维扫描的点云，生成.ply的文件，放到对应的文件夹；\n",
    "  \n",
    "\n",
    "- 选定世界坐标系，利用棋盘格进行手眼标定，根据机械臂返回的位姿对 $4*4$ 相机外矩阵进行实时解算输出到.txt文件，便于Align3D调用；\n",
    "\n",
    "- 物体位姿信息发布[publish]给机械臂的控制节点，机械臂配合进行运动规划，执行抓取；\n",
    "\n",
    "**P.S.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. 技术要点\n",
    "##### 1. ROS sevice;(save image, seg images, 3D Align);\n",
    "\n",
    "##### 2. 同步的相机节点主题读取 Multiple topics subscribed at the same time;\n",
    "\n",
    "##### 3. Seg2D 分割\n",
    "参考的demo中使用[FCN](https://blog.csdn.net/scutjy2015/article/details/77358172), [论文出处](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf),可以对于数据集中要标注的物体每一个都可以生成一个热点图来表示对应区域是这个物体的概率，这里我们采用的是[Mask-RCNN](https://blog.csdn.net/linolzhang/article/details/71774168), [论文出处]（https://arxiv.org/abs/1703.06870） 目前选择的已经训练好的模型会对[COCO Dataset 2017]（http://cocodataset.org/#home）  包括常见的近40种物体，这里我们只关心可以被抓取的物体（手提包，背包，领带，球，酒杯，瓶子，茶杯，叉子，刀叉，勺子，香蕉，苹果，橙子，鼠标，手机，遥控器，书，剪刀，牙刷，玩具熊，吹风机）会同时生成对物体检测的限位框[bounding box]，识别出的物体类别以及概率[confidence/score]，以及分割好的掩板;\n",
    "PS：如果需要识别新的物体，或者对于某些物体进行精确的分割，需要采集获取新的数据集，并对模型进行重新训练，我对原demo里的单物品多位姿摆放的数据集有做过训练，在多物品的测试图片上测试效果并不是很好；多物品摆放的数据集非常重要，比如[FAT](), 另外重新训练，可以参照这个[教程](https://blog.csdn.net/l297969586/article/details/79140840)\n",
    "Some tips on training Seg2D \n",
    "\n",
    "##### 4. Align3D, Point cloud reconstruction, ICP;\n",
    "采集好多个视角[view]的彩色图与深度图（aligned depth image, 640 x 480），配和彩色相机内参[color-intristics],深度相机内参[deth intristics]转到相机坐标系，根据深度值>0, 进行第一次滤波，然后根据每个视角[view]的相机外参[camera-to-world]的, 以及盒子相对于世界的转换矩阵[bin-to-world]，依次把每一个视角生成的点云添加到一起，生成一个总的点云1(3*N, N 表示点云中体素数)；\n",
    "\n",
    "导入[load]预先采集好的背景[background]点云2(这里指的就是盒子[bin]预先扫描生成的点云)，对点云1和点云2（两个点云均相对于盒子）进行进一步的精确匹配，使得全部较好地匹配到盒子[bin]的坐标系；\n",
    "\n",
    "对于每个种类的物体，遍历多个视角获取的rgb-d图，把利用卷积神经网络分割好的二值图像[binary image]掩板[Mask]与深度数据进行相乘，这样深度图数据里就只有单个物体对应的点云，按照一的方法，就可以生成单个种类物体的单独点云[point cloud, PCL]；要注意\n",
    " - 参考的demo里，是对每一个物体都会生成一个可能区域的热点图[heatmap]，在多个view综合后取算数平均值与方差确定二值分割的阈值[threshold],然后生成前面用到的二值图像掩板，在我们的模型里，返回的是各个物体自己的二值化掩板以及整个区域对应是某种物体的概率，另外在确定实际演示的物体后，要去掉模型对不需要物体的分割效果；如果是直接使用demo，可以重新根据概率生成对于每个物体的各个视角下的热点图，或者把demo里的获取Mask的接口函数进行调整；\n",
    " - 生成的物体的点云，会有一些背景的噪点，所以还要减掉之前配准好的背景点云；\n",
    " - 某种品类的物体可能会有多个实体[instance], 所以在这一步的时候，还会根据已知的该类物体的个数来做一个K-means分类，这样就可以把某种物体的点云集合分割成单独的实体，从而得到多个实体各自对应的点云，以及中心点；\n",
    " - 稳定[stable]而且好[good]的掩板分割对后面的三维重建与分割非常关键\n",
    " \n",
    "对于上一步中分割好的某种物体的各个实体，又会开始进行遍历，\n",
    "进一步去噪，然后减采样获得跟物体模型相近精度的点云，再通过通过计算表面均值[surface mean]以及PCA获得初始的位姿计算结果，在ICP中迭代获得最终的位姿计算结果；\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Demo2 初步\n",
    " \n",
    "ICRA 2018 Paper [Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching](https://arxiv.org/pdf/1710.01330.pdf) \n",
    "\n",
    "[项目网站，**快来点我** ~][hover]\n",
    "[hover]: http://arc.cs.princeton.edu/ \"好，现在单击吧\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix \n",
    "#### 1 模型识别物体清单\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']\n",
    "               \n",
    "#### 2 相机参数\n",
    "<img src=\"images/sr300.png\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **SR300 属性**<br> . </center></caption>\n",
    "\n",
    "#### 3 环境配置脚本以及运行常见bug解决"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
